{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHj92VDfTEPO"
   },
   "source": [
    "Your name:\n",
    "\n",
    "Your student ID number:\n",
    "\n",
    "Shared link to this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoFerkqpTEPQ"
   },
   "source": [
    "# Programming Assignment 2 (P2) for COMPSCI 446, Search Engines\n",
    "\n",
    "_This is an updated version of the notebook for P2. Please watch Piazza for announcements of new versions and/or check back periodically to see if it has been updated._\n",
    "\n",
    "_As of this version of the notebook, the autograder is set up in Gradescope._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVSv_Zs7TEPR"
   },
   "source": [
    "The purpose of this project is to explore and implement some of the effectiveness measures discussed in class and in the text (Section 8.4). You will implement a function called eval that will be invoked as follows:\n",
    "```python\n",
    "eval(trecrunFile, qrelsFile, outputFile)\n",
    "```\n",
    "\n",
    "where `trecrunFile` and `qrelsFile` are files that we will provide to you and `outputFile` is where you will print the evaluation results of the trecrun file.\n",
    "\n",
    "A trecrun file contains the actual system runs that you are going to evaluate. It is a text file with six space-separated columns on every line (note that for obvious reasons, no column can contain spaces):\n",
    "- The first column is the query name (aka query id)\n",
    "- The second column is unused and should always contain \"skip\" (for historical reasons)\n",
    "- The third column is a document identifier (“docid”)\n",
    "- The fourth column is the rank of that document for that query in this run\n",
    "- The fifth column is the score from the retrieval model.\n",
    "- The sixth column is some text to describe the run itself, normally the same for every line in the file.\n",
    "\n",
    "The qrels file contains judgment information: given a query and a document, is the document relevant to the query? It is another space-separated text file:\n",
    "- The first column is the query name/id (corresponding to the query name/id in the trecrun files)\n",
    "- The second column is unused (it is present for historical reasons; you won't need to do anything with it except be sure you read it to get to the remaining columns)\n",
    "- The third column is a document identifier (“docid”)\n",
    "- The fourth column is a number representing the relevance of the document, either 0 for non-relevant, or positive for relevant.\n",
    "\n",
    "We will provide a common qrels file as well as several trecrun files and sample output files. We will also provide copies of the queries in case you find it useful, though they aren't needed.\n",
    "\n",
    "Note that for many of the query-docid pairs in the trecrun file, you will find a corresponding pair in the qrels file, helping you to evaluate how well the retrieval model did for that query. However, as discussed in class, **large numbers of query-docid pairs will be unjudged, so will not appear in the qrels file. When you encounter that, you should assume that the query-docid pair is non-relevant** (i.e., has a relevance score of zero).\n",
    "\n",
    "Your code will read in the provided trecrun and qrels files. It will then generate evaluation scores and print them in a particular format (see below) to the indicated output filename.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4Xvai7VPAdS"
   },
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "id": "hWAYQYm7AiFQ"
   },
   "outputs": [],
   "source": [
    "version = 2 # DO NOT MODIFY. If notebook does not match with autograder version, many tests are likely to fail!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaALU6IfSdcP"
   },
   "source": [
    "\n",
    "\n",
    "We first execute the following to connect to Google Drive (you will be prompted repeatedly for access to your Google Drive; please give it permission) and download copies of the sample files listed above. You should not need to make any modifications to the code, though if you want to use a slightly different path in Google Drive, you can modify the appropriate data_path value. (The autograder will not use your Google Drive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25683,
     "status": "ok",
     "timestamp": 1727148834133,
     "user": {
      "displayName": "Joanna Li",
      "userId": "00938000178745854707"
     },
     "user_tz": 240
    },
    "id": "ZjCDAZdyTEPT",
    "outputId": "b2160068-499c-45c1-a1de-7cd376524262"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    in_colab = True\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "\n",
    "\n",
    "# You are more than welcome to code some helper functions.\n",
    "# But do note that we are only grading functions that are coded in the template files.\n",
    "\n",
    "\n",
    "# Connect to Google Drive and download copies of the sample files listed above.\n",
    "# Please allow the access to your Google Drive or the following dataset loader will fail.\n",
    "# (The autograder will not use your Google Drive.)\n",
    "if in_colab:\n",
    "  drive.mount(\"/content/drive/\") ## DO NOT MODIFY THIS LINE\n",
    "  data_path = \"/content/drive/MyDrive/COMPSCI446/P2\" ## CHANGE TO YOUR OWN FOLDER ON GOOGLE DRIVE\n",
    "else:\n",
    "  data_path = \"./data/\"  ## DO NOT MODIFY THIS LINE. CHANGING THIS LINE WOULD RESULT IN FAIL OF AUTOGRADER TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoA5bSTsGl2f"
   },
   "source": [
    "We will now download the files needed for this assignment to your Google Drive in the path declared in <em>data_path</em> so that you can work with them in the rest of the notebook. We will not bother loading the files if they have already been loaded to your Google Drive, so this should be a one-time effort.\n",
    "\n",
    "**NOTE**: This code will not create a new folder for you. It assumes that you have a folder called COMPSCI446 at the top level of your Google Drive. You can create that folder and everything will work fine. If you want to create something different (e.g., COMPSCI446-P2 or COMPSCI446/P2) then create that folder and edit the data_path line in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "id": "phggEbZAH6uH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File \"P2train.zip\" already exists, not downloading.\n",
      "Unzipping \"P2train.zip\"\n",
      "File \"P2train-output.zip\" already exists, not downloading.\n",
      "Unzipping \"P2train-output.zip\"\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "def download_file(file_path: str, zip: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Download the file to proper location in mounted Google Drive.\n",
    "\n",
    "    Args:\n",
    "        file_path: the location of file we want to download\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    webloc = \"https://cs.umass.edu/~allan/cs446/\"\n",
    "\n",
    "\n",
    "    data_info = Path(data_path)\n",
    "    if not data_info.exists() or not data_info.is_dir():\n",
    "      print(f\"Google folder \\\"{data_path}\\\" is not present or not a folder. Can't download file {file_path}.\")\n",
    "      return\n",
    "\n",
    "    local_google_drive_path = os.path.join(data_path,file_path)\n",
    "    local_file = Path(local_google_drive_path)\n",
    "    if local_file.is_file():\n",
    "        print(f\"File \\\"{file_path}\\\" already exists, not downloading.\")\n",
    "    else:\n",
    "        print(f\"Cannot find \\\"{file_path}\\\" so downloading it.\")\n",
    "        urllib.request.urlretrieve(webloc + file_path, local_google_drive_path)\n",
    "        print(\"Done\")\n",
    "\n",
    "    if zip:\n",
    "      with zipfile.ZipFile(local_google_drive_path, 'r') as zip_ref:\n",
    "        print(f\"Unzipping \\\"{file_path}\\\"\")\n",
    "        zip_ref.extractall(data_path)\n",
    "\n",
    "\n",
    "\n",
    "P2_train_path = \"P2train.zip\"\n",
    "P2_output = \"P2train-output.zip\"\n",
    "\n",
    "download_file(P2_train_path)\n",
    "download_file(P2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5zVWqTc_nqB"
   },
   "source": [
    "In this assignment, we're testing only the end result of your program -- i.e., the output file -- after it runs, parses, and evaluates the `trecrun` and `qrels` file correctly. Therefore, the only function that you **MUST** have is the eval function:\n",
    "```python\n",
    "def eval(trecrunFile, qrelsFile, outputFile):\n",
    "  pass\n",
    "```\n",
    "Other than providing a functional eval() function, **you are free to define supporting functions, classes, and use standard python libraries to implement the `eval` function in any way that you like**.\n",
    "\n",
    "**However**, we know that different people have different code structures in mind, but for ease of debugging and readability purposes, we suggest you follow the provided general outline below for your program. If you prefer another way to structure the program, you can go ahead and replace the outline with yours. As mentioned, the only exception is the `eval` function where you have to make sure that you don't modify the definition at the bottom of this notebook or the expected format of the inputs/outputs (or else the autograder will fail and your grade will suffer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M78UAoC6IBXy"
   },
   "source": [
    "# 1. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr5duhuoLBzQ"
   },
   "source": [
    "## 1.1 Creating a data structure\n",
    "\n",
    "The qrels and trecrun files can get very large. Iterating through the entire file for each run will take minutes, if not longer, and therefore is impractical. In practice, it is more efficient to implement a data structure to hold `qrels` and  `trecrun` information.\n",
    "\n",
    "We'll highlight once more that this is a suggested way to handle the qrels and trecrun files. You _unquestionably_ want to store them in some data struture, but what you call it and how you use it is entirely up to you. We're providing a suggestion that may help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "id": "n0RZSkVqVaA6"
   },
   "outputs": [],
   "source": [
    "class QueryInfo:\n",
    "  \"\"\"\n",
    "  QueryInfo class: store the trecrun and qrels data of each query\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, query_id):\n",
    "    self.query_id = query_id\n",
    "    self.ignore_query = False\n",
    "    self.qrel_dict = {} # dict[doc_id: relevance]\n",
    "    self.trecrun_dict = {} # dict[doc_id: (rank, score, model)]\n",
    "\n",
    "  def add_qrel_info(self, doc_id: str, relevance: str):\n",
    "    self.qrel_dict.update({doc_id: relevance})  # checks if doc_id in qrel_dict, if not adds a new dict entry\n",
    "\n",
    "  def add_trecrun_info(self, doc_id: str, rank: str, score: str, model: str):\n",
    "    self.trecrun_dict.update({doc_id: (rank, score, model)})\n",
    "  \n",
    "  def number_relevant(self):\n",
    "    if self.ignore_query: return 0\n",
    "    count = 0\n",
    "    for relevance in self.qrel_dict.values():\n",
    "      if relevance > '0':\n",
    "        count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgQzke3mRlIP"
   },
   "source": [
    "For easier access to qrel and trecrun information during evaluation measure calculations, we suggest you store all query information in a dictonary mapping query ids to the corresponding query object. Note that this example assumes that query ids are strings (rather than integers) which is a good assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "id": "4M93-nPXR9wJ"
   },
   "outputs": [],
   "source": [
    "queries = dict[str, QueryInfo]() # dictionary to store query_id/QueryInfo mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x34MEQrcINNu"
   },
   "source": [
    "## 1.2 Parsing trecruns\n",
    "Now we can use the data structure above to store the trecrun rankings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "id": "8vOQ1zL8IZFf"
   },
   "outputs": [],
   "source": [
    "def read_trecrun(trecRunFile: str, queriesDict: dict[str, QueryInfo]) -> None:\n",
    "  \"\"\"\n",
    "  Read the trecrun file data and store the ranking lists for each query the a corresponding QueryInfo object.\n",
    "\n",
    "  Args:\n",
    "        trecRunFile: path to where the trecrun file is stored\n",
    "        queriesDict: dictionary to store QueryInfos, mapping each query's id to its corresponding QueryInfo object\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  with open(trecRunFile, \"r\") as trecrun_file:\n",
    "    for line in trecrun_file.readlines():\n",
    "      if line:\n",
    "        trecrun_line = line.split()\n",
    "        query_id = trecrun_line[0]\n",
    "        doc_id = trecrun_line[2]\n",
    "        rank = trecrun_line[3]\n",
    "        score = trecrun_line[4]\n",
    "        model = trecrun_line[5]\n",
    "        \n",
    "        if query_id in queriesDict.keys():\n",
    "          queriesDict[query_id].add_trecrun_info(doc_id=doc_id, rank=rank, score=score, model=model)\n",
    "        else:\n",
    "          queriesDict[query_id] = QueryInfo(query_id=query_id)  # make new query object\n",
    "          queriesDict[query_id].add_trecrun_info(doc_id=doc_id, rank=rank, score=score, model=model)   # add trecrun information\n",
    "\n",
    "  return\n",
    "\n",
    "query_dict = {} # new dictionary for system we're evaluating\n",
    "testing_files = [\"msmarcosmall-bm25.trecrun\", \"msmarcosmall-dpr.trecrun\", \"msmarcosmall-ql.trecrun\"]\n",
    "\n",
    "read_trecrun(trecRunFile=\"./data/\" + testing_files[1], queriesDict=query_dict)\n",
    "# # for query_id, query in query_dict.items():\n",
    "#   for doc_id, items in query.trecrun_dict.items():\n",
    "#     print(f\"query id {query_id} contains doc {doc_id} with rank {items[0]} score {items[1]} model {items[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfn5tXJUPedJ"
   },
   "source": [
    "##1.3 Parsing qrels\n",
    "Add the qrels data to the data structure as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "id": "eV9MOTaKPgqd"
   },
   "outputs": [],
   "source": [
    "def read_qrels(qrelsFile: str, queriesDict: dict[str, QueryInfo]) -> None:\n",
    "  \"\"\"\n",
    "  Read the qrels file data and store the relevance judgements for each query in a corresponding QueryInfo object.\n",
    "\n",
    "  Args:\n",
    "        qrelsFile: path to where the qrels file is stored\n",
    "        queriesDict: dictionary to store QueryInfos, mapping each query's id to its corresponding QueryInfo object\n",
    "  \"\"\"\n",
    "  \n",
    "\n",
    "  with open(qrelsFile, \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "\n",
    "      if line:\n",
    "        qrel_line = line.split()\n",
    "        query_id = qrel_line[0]   \n",
    "        doc_id = qrel_line[2]\n",
    "        relevance = qrel_line[3]\n",
    "\n",
    "        if query_id in queriesDict.keys():   # query in qrel is also in trecrun\n",
    "          queriesDict[query_id].add_qrel_info(doc_id=doc_id, relevance=relevance)\n",
    "\n",
    "\n",
    "read_qrels(qrelsFile=\"./data/msmarco.qrels\", queriesDict=query_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5x8oiqtLenP"
   },
   "source": [
    "# 2. Effectiveness measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HUL96L5L4PW"
   },
   "source": [
    "We expect you to implement the following effectiveness measures and later (in the ```eval``` function) report values on them for any trecrun file that is provided. **If the file contains multiple queries (which it usually will), you will need to calculate measures for each query and then also produce the arithmetic mean of each evaluation measure across all queries in the trecrun file.**\n",
    "\n",
    "Note that you are producing these numbers for every query that occurs in the trecrun file. There will probably be queries listed in the qrels file that are not in the trecrun file; ignore them.\n",
    "\n",
    "The measures you will need to implement for each query are (please use the formulations described in the textbook, and macro-averaging where given the choice):\n",
    "\n",
    "- `numRel` is the total number of relevant documents (relevance score above zero) that are listed in the qrels file for this query.\n",
    "- `relFound` is the count of those relevant documents that also appear in the ranked list of documents.\n",
    "- `RR` (Reciprocal Rank); if relFound is zero, then report “0” for this measure.\n",
    "- `P@13` (Precision @ 13)\n",
    "- `R@13` (Recall @ 13); if numRel is zero, then report \"0\" for this measure.\n",
    "- `F1@13`; if either precision or recall is zero, then report \"0\" for this measure.\n",
    "- `AP` (Average Precision); if numRel is zero, report \"0\".\n",
    "- `nDCG@23` (there are multi-value relevance judgments in the data – 0,1,2 – though some queries only have 0/1 judgments). If numRel is zero, then report 0 for nDCG. Hint: A key thing to note in calculating nDCG is that the \"ideal\" DCG cannot be calculated using just the retrieved documents for a given query. Think about why so you avoid that mistake (it was a midterm question, too). Note: use the DCG equation on page 319 of the textbook that just uses $rel_i$, not the equation that is in footnote 10 of page 320 that uses $2^{rel_i}-1$.\n",
    "- `BPREF`. If numRel is zero, report \"0\". Use the formula $\\frac{1}{R}\\Sigma_{d_r}(1-\\frac{N_{d_r}}{R})$.\n",
    "\n",
    "\n",
    "*Extra Credit:*\n",
    "\n",
    "- `P@29R` is precision when recall is 29 percent (note that this is percent recall and not rank 29); if numRel is zero, report \"0\". Remember that this requires that you interpolate to figure out the precision -- it is the maximum precision at any recall value $R'$ where $R' \\geq R$. See section 8.4.2 in the textbook or the content of the evaluation lecture.\n",
    "- `P@R` where $R$ is the number of relevant documents for this query; if numRel is zero, report \"0\".\n",
    "\n",
    "For the extra credit items, we are not providing sample output and when you submit your assignment, you will only be told that the autograder _found_ your extra credit (or not). As a hint, though, in the BM25 run of msmarcosmall, for query 390360 you should find that P@29R is 0.4400 and that P@R is 0.3429. And for query 118440, you should get 0.0000 and 0.0461, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfsI5msi9cMv"
   },
   "source": [
    "The following are possible stubs for the evaluation functions. One last time, note that you are not required to implement them this way, but you may find this way of thinking about it helpful.\n",
    "\n",
    "These stubs assume that you have identified the \"query info\" for the query you're considering and that you pass that in. The suggested variable ```queries``` was defined earlier as a dictionary that mapped query ids to this query information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "id": "C2pofBWoNImA"
   },
   "outputs": [],
   "source": [
    "def numRel(query: QueryInfo) -> int:\n",
    "      \"\"\"\n",
    "      Calculate total number of relevant documents that are listed in the qrels file for the given query.\n",
    "\n",
    "      Args:\n",
    "            query: input query holding the qrels and ranked list information\n",
    "\n",
    "      Returns: number of relevant documents in the corpus for this query\n",
    "      \"\"\"\n",
    "      return query.number_relevant()\n",
    "\n",
    "# for query_id, query_obj in query_dict.items():\n",
    "#       print(query_id, numRel(query=query_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "id": "w4NASCYZNOqD"
   },
   "outputs": [],
   "source": [
    "def relFound(query: QueryInfo) -> int:\n",
    "      \"\"\"\n",
    "      Calculate the number of relevant documents retrieved in the ranked list for the given query.\n",
    "\n",
    "      Args:\n",
    "            query: input query holding the qrels and ranked list information\n",
    "      Returns: number of relevant documents retrieved in the ranked list for this query\n",
    "      \"\"\"\n",
    "      count = 0\n",
    "      qrel_keys = query.qrel_dict.keys()\n",
    "      trecrun_keys = query.trecrun_dict.keys()\n",
    "      for doc_id in trecrun_keys:\n",
    "            if doc_id in qrel_keys and query.qrel_dict[doc_id] > '0':\n",
    "                  count += 1\n",
    "\n",
    "      return count\n",
    "# for q_id, q_obj in query_dict.items():\n",
    "#       print(f\"{q_id} relFound {relFound(query=q_obj)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "id": "rjaxZGv7NZKW"
   },
   "outputs": [],
   "source": [
    "def RR(query: QueryInfo) -> float:\n",
    "      \"\"\"\n",
    "      Calculate the reciprocal rank for the given query.\n",
    "\n",
    "      Args:\n",
    "            query: input query holding the qrels and ranked list information\n",
    "\n",
    "      Returns: single number representing the reciprocal rank score of this query\n",
    "      \"\"\"\n",
    "      if relFound(query=query) == 0: return 0.0000\n",
    "      for doc_id, items in query.trecrun_dict.items():\n",
    "            if doc_id in query.qrel_dict.keys() and query.qrel_dict[doc_id] > '0':\n",
    "                  return 1 / int(items[0])\n",
    "      return 0.0000\n",
    "            \n",
    "\n",
    "# for q_id, q_obj in query_dict.items():\n",
    "#       print(f\"{q_id} RR {RR(query=q_obj)}\")\n",
    "\n",
    "# for q_id, q_obj in query_dict.items():\n",
    "#       for doc_id, items in q_obj.trecrun_dict.items():\n",
    "#             print(f\"query {q_id} doc id {doc_id} with rank {items[0]} and items {items[1:-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "id": "T_fB0DDnNfgT"
   },
   "outputs": [],
   "source": [
    "def P_13(query: QueryInfo) -> float:\n",
    "      \"\"\"\n",
    "      Calculate Precision at 13 for the given query.\n",
    "\n",
    "      Args:\n",
    "            query: input query holding the qrels and ranked list information\n",
    "\n",
    "      Returns: single number representing the P@13 score of this query\n",
    "      \"\"\"\n",
    "\n",
    "      relevant_count = 0\n",
    "      total_count = 0\n",
    "      for doc_id in query.trecrun_dict.keys():   # ranked list \n",
    "            if total_count == 13:\n",
    "                  return relevant_count / total_count\n",
    "            if doc_id in query.qrel_dict.keys() and query.qrel_dict[doc_id] > '0':\n",
    "                  relevant_count += 1\n",
    "            \n",
    "            total_count += 1\n",
    "\n",
    "# for q_id, q_obj in query_dict.items():\n",
    "#       print(f\"P@13 for query {q_id} {P_13(query=q_obj)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "id": "rp41Qq_lNgD1"
   },
   "outputs": [],
   "source": [
    "def R_13(query: QueryInfo) -> float:\n",
    "      \"\"\"\n",
    "      Calculate Recall at 13 for the given query.\n",
    "\n",
    "      Args:\n",
    "            query: input query holding the qrels and ranked list information\n",
    "\n",
    "      Returns: single number representing the R@13 score of this query\n",
    "      \"\"\"\n",
    "      if numRel(query=query) == 0: return 0.0000\n",
    "      \n",
    "      relevant_count = 0\n",
    "      total_relevant_count = sum(1 for relevance in query.qrel_dict.values() if relevance > '0') # get total amount of relevant documents for this query\n",
    "      for i, doc_id in enumerate(query.trecrun_dict.keys()):   # ranked list \n",
    "            if i == 13:\n",
    "                  return relevant_count / total_relevant_count\n",
    "            \n",
    "            if doc_id in query.qrel_dict.keys() and query.qrel_dict[doc_id] > '0':\n",
    "                  relevant_count += 1\n",
    "            \n",
    "# for q_id, q_obj in query_dict.items():\n",
    "#       print(f\"R@13 for query {q_id} {R_13(query=q_obj)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "id": "rgQTOa9hNgjZ"
   },
   "outputs": [],
   "source": [
    "def F1_13(query: QueryInfo) -> float:\n",
    "      \"\"\"\n",
    "      Calculate F1 score at 13 for the given query.\n",
    "\n",
    "      Args:\n",
    "            query: input query holding the qrels and ranked list information\n",
    "\n",
    "      Returns: single number representing the F1@13 score of this query\n",
    "      \"\"\"\n",
    "      R = R_13(query=query)\n",
    "      P = P_13(query=query)\n",
    "      if (R or P) == 0: return 0.0000\n",
    "      \n",
    "      return (2 * R * P) / (R + P)\n",
    "\n",
    "\n",
    "# for q_id, q_obj in query_dict.items():\n",
    "#       print(f\"F1@13 for query {q_id} {F1_13(query=q_obj)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "id": "XbIlYp8FNhgt"
   },
   "outputs": [],
   "source": [
    "def AP(query: QueryInfo) -> float:\n",
    "      \"\"\"\n",
    "      Calculate Average Precision for the given query.\n",
    "\n",
    "      Args:\n",
    "            query: input query holding the qrels and ranked list information\n",
    "\n",
    "      Returns: single number representing the average precision score of this query\n",
    "      \"\"\"\n",
    "      if numRel(query=query) == 0: return 0.000\n",
    "\n",
    "      total_relevant_count = sum(1 for relevance in query.qrel_dict.values() if relevance > '0') # get total amount of relevant documents for this query\n",
    "      precision_lst = []\n",
    "      trecrun_keys = query.trecrun_dict.keys() \n",
    "      relevant_count = 0\n",
    "\n",
    "      for i, doc_id in enumerate(trecrun_keys, start=1):\n",
    "            if doc_id in query.qrel_dict.keys() and query.qrel_dict[doc_id] > '0':\n",
    "                  relevant_count += 1\n",
    "                  precision_lst.append(relevant_count / i)\n",
    "      return sum(precision_lst) / total_relevant_count\n",
    "\n",
    "# for q_id, q_obj in query_dict.items():\n",
    "#       print(f\"AP for query {q_id} {AP(query=q_obj)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "id": "44Z9r144fwNB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 47210 has NCDG_23 0.6862812123397254\n",
      "query 118440 has NCDG_23 0.2077491667557792\n",
      "query 141630 has NCDG_23 0.6108916131192492\n",
      "query 390360 has NCDG_23 0.5856050227907519\n",
      "query 555530 has NCDG_23 0.6525901813275917\n",
      "query 673670 has NCDG_23 0.026426467630837266\n",
      "query 938400 has NCDG_23 0.7582825325993713\n",
      "query 1064670 has NCDG_23 0.37583501064400027\n",
      "query 1071750 has NCDG_23 0.42169713111995905\n",
      "query 1115210 has NCDG_23 0.21865762549612613\n",
      "query 1116380 has NCDG_23 0.06977935084598216\n",
      "query 1127540 has NCDG_23 0.6790384297077041\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def dcg_at_r(relevance_lst: list, r: int) -> float:\n",
    "      if r > len(relevance_lst):\n",
    "            r = len(relevance_lst)\n",
    "      \n",
    "      dcg = relevance_lst[0]\n",
    "      for i in range(1, r):  # r-1 here because we include the first element,  relevance_lst[0]\n",
    "            dcg += (relevance_lst[i] / math.log2(i+1))\n",
    "      return dcg\n",
    "\n",
    "def NDCG_23(query: QueryInfo) -> float:\n",
    "      \"\"\"\n",
    "      Calculate nDCG at 23 for the given query.\n",
    "\n",
    "      Args:\n",
    "            query: input query holding the qrels and ranked list information\n",
    "\n",
    "      Returns: single number representing the nDCG at 23 score of this query\n",
    "      \"\"\"\n",
    "      if numRel(query=query) == 0: return 0.000 # cover edge case\n",
    "      relevance_lst = [int(query.qrel_dict[doc_id]) if doc_id in query.qrel_dict.keys() else 0 for doc_id in query.trecrun_dict.keys()] # maybe different models are ranking their documents differentlly? like dpr and ql are different than bm25\n",
    "      if len(relevance_lst) < 23:\n",
    "            while len(relevance_lst) < 23:\n",
    "                  relevance_lst.append(0)\n",
    "\n",
    "      ideal_relevance_lst = [int(relevance) for relevance in sorted(query.qrel_dict.values(), reverse=True)]\n",
    "      dcg = dcg_at_r(relevance_lst=relevance_lst, r=23)\n",
    "      idcg = dcg_at_r(relevance_lst=ideal_relevance_lst, r=23)\n",
    "      return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "for query in query_dict.values():\n",
    "      print(f\"query {query.query_id} has NCDG_23 {NDCG_23(query=query)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "id": "7AGJBvf43aB7"
   },
   "outputs": [],
   "source": [
    "def BPREF(query: QueryInfo) -> float:\n",
    "      \"\"\"\n",
    "      Calculate BPREF for the given query.\n",
    "\n",
    "      Args:\n",
    "            query: input query holding the qrels and ranked list information\n",
    "\n",
    "      Returns: single number representing the BPREF score of this query\n",
    "      \"\"\"\n",
    "      bpref = 0.0000\n",
    "      if numRel(query=query) == 0: return bpref\n",
    "\n",
    "      R = numRel(query=query)\n",
    "      for doc_id, relevance in query.qrel_dict.items():\n",
    "            if relevance > '0' and doc_id in query.trecrun_dict.keys():\n",
    "                  rank_at_dr = int((query.trecrun_dict[doc_id])[0])\n",
    "                  N_dr = min(len([1 for doc_id, items in query.trecrun_dict.items() if int(items[0]) < rank_at_dr and (doc_id not in query.qrel_dict.keys() or (doc_id in query.qrel_dict.keys() and query.qrel_dict[doc_id] <='0')) ]), R)\n",
    "                  bpref += (1 - (N_dr / R))\n",
    "\n",
    "      return bpref / R\n",
    "\n",
    "\n",
    "# for query in query_dict.values():\n",
    "#       print(f\"query {query.query_id} has bpref {BPREF(query=query)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tev-pjuNFDC"
   },
   "source": [
    "These next two stubs are for the optional extra credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "id": "slEgWM4a0xzG"
   },
   "outputs": [],
   "source": [
    "def P_29R(query: QueryInfo) -> float:\n",
    "  \"\"\"\n",
    "  (Extra Credit) Calculate Precision at 29% recall for the given query.\n",
    "\n",
    "  Args:\n",
    "        query: input query holding the qrels and ranked list information\n",
    "\n",
    "  Returns: single number representing the Precision at 29% recall score of this query\n",
    "  \"\"\"\n",
    "\n",
    "  #########\n",
    "  ##\n",
    "  ## Implement the function here\n",
    "  ##\n",
    "  #########\n",
    "\n",
    "  return NotImplemented # You will return something before this statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "id": "b3ALpuqlNKkj"
   },
   "outputs": [],
   "source": [
    "def PatR(query: QueryInfo, R: int) -> float:\n",
    "      \"\"\"\n",
    "      (Extra Credit) Calculate Precision at the number of relevant documents for the given query.\n",
    "\n",
    "      Args:\n",
    "            query: input query holding the qrels and ranked list information\n",
    "\n",
    "      Returns: single number representing the Precision at R score of this query\n",
    "      \"\"\"\n",
    "      # im assuming that 'R' is the rank of which we want the precision of the query\n",
    "      relevant_count = 0\n",
    "      total_count = 0\n",
    "\n",
    "      for doc_id in query.trecrun_dict.keys():   # ranked list \n",
    "            if total_count == R:\n",
    "                  return relevant_count / total_count\n",
    "            \n",
    "            if doc_id in query.qrel_dict.keys() and query.qrel_dict[doc_id] > '0': # document is relevant, increase this count\n",
    "                  relevant_count += 1\n",
    "            total_count += 1\n",
    "      \n",
    "\n",
    "# R_values = [13, 23, 6, 32, 69, 46, 89]\n",
    "# for query in query_dict.values():\n",
    "#       print(f\"query {query.query_id} with P@R R={R_values[6]}: {PatR(query=query, R=R_values[6])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6La4ZOuMO1BE"
   },
   "source": [
    "# 3. Evaluation and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtyopPnrSaJU"
   },
   "source": [
    "For each input trecrun file, we want you to report the calculated measures in the output file following the format given below. If the trecrun file contains multiple queries, you will need to calculate and report measures for each query. At the end of the evaluation output file, you should also include the _sum_ of the values for numRel and relFound, but the (arithmetic) _mean_ value of each of the other measures, averaged (or summed) across all queries in the trecrun file. In each case you will calculate scores per query and the mean or summed scores (so, for example, you will calculate AP for each query and the mean AP -- i.e., MAP -- for each TREC run overall). Your output format for each query should be exactly as follows, except that the amount of whitespace you use to separate fields is your choice. Each line will look like:\n",
    "```\n",
    "measure    queryid    score\n",
    "```\n",
    "Where measure is one of those given above. For most measures, format the scores to include exactly four digits after the decimal place. For counts, just report the integer number. For example, for query 302 you might get (these numbers are random, so not meaningful; plus the last two are extra credit, so you might not have them):\n",
    "```\n",
    "\n",
    "numRel        302    23\n",
    "relFound      302    20\n",
    "RR            302    1.0000\n",
    "P@13          302    0.6667\n",
    "R@13          302    0.1846\n",
    "F1@13         302    0.2667\n",
    "NDCG@23       302    0.3118\n",
    "AP            302    0.1531\n",
    "BPREF         302    0.3333\n",
    "P@29R         302    0.5000\n",
    "P@R           302    0.1111\n",
    "```\n",
    "For the final average across queries, replace RR with MRR and AP with MAP since those are the normal names for the means. Also, for the final averages, use \"all\" in place of the queryname. For example,\n",
    "```\n",
    "MRR           all   0.9438\n",
    "MAP           all   0.0581\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "id": "z_F6VyrgDKWg"
   },
   "outputs": [],
   "source": [
    "def report_measures(input_trecrun: str, input_qrel: str, outputFile: str, results: list) -> None:\n",
    "      \"\"\"\n",
    "      Write all calculated measures into the output file.\n",
    "\n",
    "      Args:\n",
    "            input_trecrun: path where the trecrun file is located\n",
    "            input_qrel: path where the qrel file is located\n",
    "            outputFile: path in which the output file should be stored\n",
    "            results: contains all calculated measures for all queries in the trecrun file\n",
    "      \"\"\"\n",
    "      report_measure_dict = {}\n",
    "\n",
    "      read_trecrun(trecRunFile=input_trecrun, queriesDict=report_measure_dict)\n",
    "      read_qrels(qrelsFile=input_qrel, queriesDict=report_measure_dict)\n",
    "\n",
    "      ndcg_at_23_value = 0\n",
    "      num_rel_value = 0\n",
    "      rel_found_value = 0\n",
    "      rr_value = 0\n",
    "      p_at_13_value = 0\n",
    "      r_at_13_value = 0\n",
    "      f1_at_13_value = 0\n",
    "      ap_value = 0\n",
    "      bpref_value = 0\n",
    "      for query in report_measure_dict.values():\n",
    "            ndcg_at_23_value += NDCG_23(query=query)\n",
    "            num_rel_value += numRel(query=query)\n",
    "            rel_found_value += relFound(query=query)\n",
    "            rr_value += RR(query=query)\n",
    "            p_at_13_value += P_13(query=query)\n",
    "            r_at_13_value += R_13(query=query)\n",
    "            f1_at_13_value += F1_13(query=query)\n",
    "            ap_value += AP(query=query)\n",
    "            bpref_value += BPREF(query=query)\n",
    "            \n",
    "            results.append([\n",
    "                  [\"NDCG@23\", query.query_id, round(NDCG_23(query=query), 4)],\n",
    "                  [\"numRel\", query.query_id, numRel(query=query)],\n",
    "                  [\"relFound\", query.query_id, relFound(query=query)],\n",
    "                  [\"RR\", query.query_id, round(RR(query=query), 4)],\n",
    "                  [\"P@13\", query.query_id, round(P_13(query=query), 4)],\n",
    "                  [\"R@13\", query.query_id, round(R_13(query=query), 4)],\n",
    "                  [\"F1@13\", query.query_id,  round(F1_13(query=query), 4)],\n",
    "                  [\"AP\", query.query_id, round(AP(query=query), 4)],\n",
    "                  [\"BPREF\", query.query_id, round(BPREF(query=query), 4)]\n",
    "            ])\n",
    "\n",
    "      number_of_queries = len(results)\n",
    "\n",
    "      final_metrics = [\n",
    "            [\"NDCG@23\", \"all\", round(ndcg_at_23_value/number_of_queries, 4) if number_of_queries != 0 else 0.000],\n",
    "            [\"numRel\", \"all\", num_rel_value],\n",
    "            [\"relFound\", \"all\", rel_found_value],\n",
    "            [\"MRR\", \"all\", round(rr_value/number_of_queries, 4) if number_of_queries != 0 else 0],\n",
    "            [\"P@13\", \"all\", round(p_at_13_value/number_of_queries, 4) if number_of_queries != 0 else 0.000],\n",
    "            [\"R@13\", \"all\", round(r_at_13_value/number_of_queries, 4) if number_of_queries != 0 else 0.000], \n",
    "            [\"F1@13\", \"all\", round(f1_at_13_value/number_of_queries, 4) if number_of_queries != 0 else 0.000], \n",
    "            [\"MAP\", \"all\", round(ap_value/number_of_queries, 4) if number_of_queries != 0 else 0.000],\n",
    "            [\"BPREF\", \"all\", round(bpref_value/number_of_queries, 4) if number_of_queries != 0 else 0.000]\n",
    "      ]\n",
    "\n",
    "      with open(outputFile, 'w') as output:\n",
    "            for query_metrics in results:\n",
    "                  for metric in query_metrics:\n",
    "                        measure = metric[0]\n",
    "                        queryid = metric[1]\n",
    "                        score = metric[2]\n",
    "                        if type(score) == float:\n",
    "                              output.write(f'{measure:<8} {queryid:<8} {score:<8.4f} \\n')\n",
    "                        else:\n",
    "                              output.write(f'{measure:<8} {queryid:<8} {score:<8} \\n')\n",
    "\n",
    "            for metric in final_metrics:\n",
    "                  measure = metric[0]\n",
    "                  queryid = metric[1]\n",
    "                  score = metric[2]\n",
    "                  if type(score) == float:\n",
    "                        output.write(f'{measure:<8} {queryid:<8} {score:<8.4f} \\n')\n",
    "                  else:\n",
    "                        output.write(f'{measure:<8} {queryid:<8} {score:<8} \\n')\n",
    "            \n",
    "            output.close()\n",
    "                        \n",
    "\n",
    "# test_trecrun = \"./data/msmarcosmall-bm25\"\n",
    "# test_qrel = \"./data/msmarco\"\n",
    "# output_file_location = \"./data/bm25.myoutput\"\n",
    "# result_lst = []\n",
    "# report_measures(test_trecrun, test_qrel, output_file_location, result_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R48sIqT3O7zJ"
   },
   "source": [
    "## Putting it all together\n",
    "Use all the definitions and functions you implemented above to complete the definition of the  `eval` function, regardless of whether you used our suggestions or structured things in your own way.\n",
    "\n",
    "Remember that the expected behavior of the `eval` function is to take as input paths to trecrun and qrel files, calculate all the measures for each query in the trecrun file, and report the calculated measures and their averages in an output file in the path included in the input to `eval`.\n",
    "\n",
    "Make sure not to modify the definition of the `eval` function or the expected format of the inputs/outputs (otherwise the autograder will fail). The body of the ``eval`` function will depend on how you implemented the details above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "id": "bNgda_3eR8VT"
   },
   "outputs": [],
   "source": [
    "def eval(trecrunFile: str, qrelsFile: str, outputFile: str) -> None:\n",
    "      \"\"\"\n",
    "      Calculate and report all the measures for each query in the trecrun file.\n",
    "\n",
    "      Args:\n",
    "            trecRunFile: path to where the trecrun file is stored\n",
    "            qrelsFile: path to where the qrels file is stored\n",
    "            outputFile: path in which the output file should be stored\n",
    "      \"\"\"\n",
    "      result_lst = []\n",
    "      report_measures(input_trecrun=trecrunFile, input_qrel=qrelsFile, outputFile=outputFile, results=result_lst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U1WnOJdSVZv"
   },
   "source": [
    "### Run the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybLuRBppHk0l"
   },
   "source": [
    "Now we can run the evaluation on the trecruns provided. Here's a breakdown of all the files we previously downloaded for this assignment. First, the `P2train.zip` file contains:\n",
    "\n",
    "- `msmarco.queries` is the actual queries that were used to generate the run files provided here and for which the qrels indicate what is relevant. You may find this useful or you may not. You do not need to use the file for the evaluation itself. It is provided in case you are curious.\n",
    "- `msmarco.qrels` is the list of which documents are relevant to which queries in the format described earlier. This file should be used for all of your evaluations in this project. Note that some queries will appear in this file but may not be in the trecrun output. In that case your evaluation output should not list those queries.\n",
    "- `msmarcosmall-{ql,bm25,dpr}.trecrun` are three ranked list files that you can use to try out your code.\n",
    "- `msmarcofull-{ql,bm25,dpr}.trecrun` are three files that you will evaluate without having the output provided to you. It is a superset of `msmarcosmall-*.trecrun`, so you could check that you get the same answers on the queries that are copied over.\n",
    "\n",
    "The file `P2train-output.zip` contains:\n",
    "- `msmarcosmall-{ql,bm25,dpr}.expeval` are the three output files corresponding to `msmarcosmall*.trecrun` showing what the output for each of those should be.\n",
    "\n",
    "Note that all of these files should have been successfully downloaded and unzipped in your `data_path` directory very early in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "id": "jvgmtA2JDSuJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25.myoutput\n",
      "empty-p2-output.txt\n",
      "msmarco.qrels\n",
      "msmarco.queries\n",
      "msmarcofull-bm25.expeval\n",
      "msmarcofull-bm25.myoutput\n",
      "msmarcofull-bm25.trecrun\n",
      "msmarcofull-dpr.expeval\n",
      "msmarcofull-dpr.myoutput\n",
      "msmarcofull-dpr.trecrun\n",
      "msmarcofull-ql.expeval\n",
      "msmarcofull-ql.myoutput\n",
      "msmarcofull-ql.trecrun\n",
      "msmarcosmall-bm25.expeval\n",
      "msmarcosmall-bm25.myoutput\n",
      "msmarcosmall-bm25.trecrun\n",
      "msmarcosmall-dpr.expeval\n",
      "msmarcosmall-dpr.myoutput\n",
      "msmarcosmall-dpr.trecrun\n",
      "msmarcosmall-ql.expeval\n",
      "msmarcosmall-ql.myoutput\n",
      "msmarcosmall-ql.trecrun\n",
      "P2train-output.zip\n",
      "P2train.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "qrels_file = \"\"\n",
    "trecrun_files = []\n",
    "expected_output_files = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(data_path):\n",
    "    if os.path.isfile(os.path.join(data_path, filename)):\n",
    "\n",
    "        if filename.endswith('.qrels'):\n",
    "          qrels_file = os.path.join(data_path, filename)\n",
    "        if filename.endswith('.trecrun'):\n",
    "          trecrun_files.append(os.path.join(data_path, filename))\n",
    "        if filename.endswith('.expeval'):\n",
    "          expected_output_files.append(os.path.join(data_path, filename))\n",
    "\n",
    "        print(filename)\n",
    "\n",
    "# print(qrels_file)\n",
    "# print(trecrun_files)\n",
    "# print(expected_output_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-ULkNHxNnno"
   },
   "source": [
    "Next we can run the `eval` function for all trecrun files downloaded. (During debugging you may want to do just one of the files until things work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "id": "NBWNO1O7STn0"
   },
   "outputs": [],
   "source": [
    "for trecrun_file in trecrun_files:\n",
    "  run_name = os.path.splitext(trecrun_file)[0]\n",
    "  output_file = run_name + '.expeval' # change this back to .expval when submitting\n",
    "  eval(trecrun_file, qrels_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdxnIxWKMlka"
   },
   "source": [
    "You are encouraged to write additional code to test the output of your evaluation function and compare with the expected output files (or manually do so) before submitting to Gradescope. In particular, you might want to consider edge cases where something odd (but technically correct) is happening.\n",
    "\n",
    "Note that generating the output files is to help you debug your overall evaluation processing. The autograder will regenerate all of the output files itself, including some on data that you do not have access to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP8AeMh6wyD4"
   },
   "source": [
    "# 4. Analysis questions\n",
    "\n",
    "## 4.1 Comparison table\n",
    "Create a table of the \"full\" QL, BM25, and DPR runs where the table has six columns as follows:\n",
    "\n",
    "* Column one is a query number\n",
    "* Column two is that query's AP performance on BM25 (%6.f)\n",
    "* Column three is that query's AP for that query in QL (%6.4f)\n",
    "* Column four is the percent improvement from BM25 to QL (%5.1f)\n",
    "* Column five is the query's performance in DPR (%6.4f)\n",
    "* Column six is the percent improvement from BM25 to DPR (%5.1f)\n",
    "\n",
    "You can write whatever function you like to do this (we will grade the output and not test your function directly). It might be simplest to read in multiple trecrun files and evaluate them each to generate this information, most likely using your existing evaluation code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "id": "183o9Q25yctg"
   },
   "outputs": [],
   "source": [
    "def comparison_table():\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoKzXJNP1XXh"
   },
   "source": [
    "## 4.2 Interpret comparison table\n",
    "What does that table tell you? We are in the process of talking about these models in class, but in case we haven't gotten to them or in case you forget... To a first approximation: QL is a straightforward estimate of term probability in a document; BM25 is a more complicated combination of features such as the frequency of a term in the document and across the collection, document length, and so on; and that DPR is the dense passage retrieval neural model. If it is helpful, the textbook talks about QL and BM25; it does not talk about DPR.\n",
    "\n",
    "Speculate on the reason for the results you are seeing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgCAYjTz12sh"
   },
   "source": [
    "*Enter your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZrmnyDnyo6Q"
   },
   "source": [
    "## 4.3 AP with no documents\n",
    "\n",
    "How do you calculate AP of a query with no retrieved documents? (This is different than a query that retrieves only some of the relevant documents. Here, it retrieves _nothing_!) Why is that a hard question? Argue for what you think the answer should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_da0nk1y9ut"
   },
   "source": [
    "*Enter your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Mgu_ih9zMV3"
   },
   "source": [
    "## 4.4 Recall / precision graph\n",
    "\n",
    "Plot a precision / recall graph for query 141630. Include data for QL, BM25, and DPR on the same graph. Note that there is not enough information to do this in the output described above -- you will have to generate additional information during the evaluation of query 141630 to accomplish this. You do not have to interpolate the values for this graph; you can plot them directly.\n",
    "\n",
    " We are providing you with the graphing code from P1's analysis questions. You may adapt it to this problem or you may use a completely different function (including one of your own!) if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "id": "R2uzyGDuzlh1"
   },
   "outputs": [],
   "source": [
    "# Graphing code\n",
    "\n",
    "# Produce the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VQR0VIdz3kM"
   },
   "source": [
    "## 4.5 Recall / precision interpolation (extra credit)\n",
    "\n",
    "Repeat 4.4 but graph it for recall values of 0.0 through 1.0 at intervals of 0.1, with the precision values interpolated as presented in class.\n",
    "\n",
    "Depending on how you implemented the graphing function for 4.4, you may be able to use the same one you used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "id": "LMBEJJBr0UQn"
   },
   "outputs": [],
   "source": [
    "# Maybe a new graphing function\n",
    "\n",
    "# Produce the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92M-bw8hTEPd"
   },
   "source": [
    "# 5. Misc & Grading\n",
    "\n",
    "**WARNING: Points breakdown may shift with a later version. This gives you an idea of what we are expecting to do.**\n",
    "\n",
    "Your P2 submission is graded out of 100 points, allocated as follows:\n",
    "* 75 points for the code (these are autograded)\n",
    "  * 6 points: 3 points each for correctly calculating `numRel` and `relFound` on all six run files\n",
    "  * 24 points: 6 points each for correctly calculating `RR`, `P@13`, `R@13`, and `F1@13` on all six run files\n",
    "  * 45 points: 15 points each for correctly calculating `NDCG@23`, `AP`, and `BPREF` on all six run files\n",
    "  * Up to 6 points extra credit for P@29R and P@R on all six run files\n",
    "\n",
    "* 25 points for the analysis questions (these are graded manually):\n",
    "  * 10 points for correctness of generated table (4.1)\n",
    "  * 2 points for interpretation of generated table (4.2)\n",
    "  * 3 points for discussion of MAP (4.3)\n",
    "  * 10 points for correctness of recall/precision graph (4.4)\n",
    "  * Up to 4 points for extra credit interpolation graph (4.5)\n",
    "\n",
    "Note that we expect that you will upload your submission in the correct format (notebook and PDF), that the code in the notebook will run, and that the code will successfully process provided trecrun files, possibly including some you do not have access to."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
